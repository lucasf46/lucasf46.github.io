{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Perceptron Blog Post\n",
    "author: Lucas Flemming\n",
    "date: '2023-01-10'\n",
    "image: \"Perceptron.jpg\"\n",
    "description: \"Implementing the Perceptron algorithm\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LINK TO PERCEPTRON ON GITHUB: \n",
    "\n",
    "https://github.com/lucasf46/lucasf46.github.io/blob/main/posts/BlogPost3/perceptron.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION:\n",
    "\n",
    "The purpose of this blog post is to implement the perceptron algorithm and to understand how the algorithm runs with various types of data.\n",
    "In this particular case, there are three types of data the perceptron runs on: linearly separable data, non-linearly separable data, and data with more than 2 features. The first way to visualize the effectiveness of the perceptron is to track the algorithms calculation of loss as it searches for an ideal weight vector. A second way is to see the actual classification line shift on top of the graphed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERCEPTRON GRAD:\n",
    "\n",
    "The perceptron.grad function takes the feature matrix X and the target vector y as arguments. First, the function\n",
    "computes the score for each element x in the data with a weight w. The value of y in this binary classification \n",
    "problem can be either -1 or 1. Therefore, within the function's return statement, the predicted score of the ith element \n",
    "is multiplied the yi which is the true label of that ith element. If the value is less than 0, that means the predicted\n",
    "and actual values are different, indicating a misclassification. Multiplying that result by the product of X and y\n",
    "sets the elements corresponding to misclassified samples to their original values, while setting the correctly classified\n",
    "elements to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Implement Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from perceptron import Perceptron, PerceptronOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n",
    "    \n",
    "    y = torch.arange(n_points) >= int(n_points/2)\n",
    "    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n",
    "    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n",
    "\n",
    "    # convert y from {0, 1} to {-1, 1}\n",
    "    y = 2*y - 1\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = perceptron_data(n_points = 300, noise = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perceptron_data(X, y, ax):\n",
    "    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n",
    "    targets = [-1, 1]\n",
    "    markers = [\"o\" , \",\"]\n",
    "    for i in range(2):\n",
    "        ix = y == targets[i]\n",
    "        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n",
    "    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_line(w, x_min, x_max, ax, **kwargs):\n",
    "    w_ = w.flatten()\n",
    "    x = torch.linspace(x_min, x_max, 101)\n",
    "    y = -(w_[0]*x + w_[2])/w_[1]\n",
    "    l = ax.plot(x, y, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "X, y = perceptron_data()\n",
    "plot_perceptron_data(X, y, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRAPHING LOSS FUNCTION OVER TIME:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a model and an optimizer\n",
    "p = Perceptron() \n",
    "opt = PerceptronOptimizer(p)\n",
    "\n",
    "loss = 1.0\n",
    "\n",
    "max_iterations = 100\n",
    "iterations = 0\n",
    "\n",
    "# for keeping track of loss values\n",
    "loss_vec = []\n",
    "\n",
    "n = X.size()[0]\n",
    "\n",
    "loss_threshold = 0.01\n",
    "\n",
    "#while loss > 0: # dangerous -- only terminates if data is linearly separable\n",
    "while iterations < max_iterations:\n",
    "    # not part of the update: just for tracking our progress \n",
    "\n",
    "    if (loss < loss_threshold):\n",
    "        break\n",
    "\n",
    "    loss = p.loss(X, y) \n",
    "    loss_vec.append(loss)\n",
    "\n",
    "    # pick a random data point\n",
    "    i = torch.randint(n, size = (1,))\n",
    "    x_i = X[[i],:]\n",
    "    y_i = y[i]\n",
    "                \n",
    "    # perform a perceptron update using the random data point\n",
    "    opt.step(x_i, y_i)\n",
    "\n",
    "    iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_vec, color = \"slategrey\")\n",
    "plt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\n",
    "labs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZING PERCEPTRON ALGORITHM OVER SEVERAL ITERATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234567)\n",
    "\n",
    "# initialize a perceptron \n",
    "p1 = Perceptron()\n",
    "opt1 = PerceptronOptimizer(p1)\n",
    "p1.loss(X, y)\n",
    "\n",
    "# set up the figure\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 5)\n",
    "fig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\n",
    "markers = [\"o\", \",\"]\n",
    "marker_map = {-1 : 0, 1 : 1}\n",
    "\n",
    "# initialize for main loop\n",
    "current_ax = 0\n",
    "loss = 1\n",
    "loss_vec = []\n",
    "\n",
    "while loss > 0:\n",
    "\n",
    "    ax = axarr.ravel()[current_ax]\n",
    "\n",
    "    # save the old value of w for plotting later\n",
    "    old_w = torch.clone(p1.w)\n",
    "\n",
    "    # make an optimization step -- this is where the update actually happens\n",
    "    # now p.w is the new value \n",
    "\n",
    "    #i, local_loss = opt.step(X, y)\n",
    "    i = torch.randint(n, size = (1,))\n",
    "    x_i = X[[i],:]\n",
    "    y_i = y[i]\n",
    "                \n",
    "    # perform a perceptron update using the random data point\n",
    "    local_loss = opt1.step(x_i, y_i)\n",
    "\n",
    "    # if a change was made, plot the old and new decision boundaries\n",
    "    # also add the new loss to loss_vec for plotting below\n",
    "    if local_loss > 0:\n",
    "        plot_perceptron_data(X, y, ax)\n",
    "        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n",
    "        loss = p1.loss(X, y).item()\n",
    "        loss_vec.append(loss)\n",
    "        draw_line(p1.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n",
    "        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n",
    "        # draw_line(w, -10, 10, ax, color = \"black\")\n",
    "        ax.set_title(f\"loss = {loss:.3f}\")\n",
    "        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n",
    "        current_ax += 1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NON-LINEARLY SEPARABLE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_perceptron_data(n_points = 300, noise = 0.5, p_dims = 2):\n",
    "    \n",
    "    y = torch.arange(n_points) >= int(n_points/2)\n",
    "    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n",
    "    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n",
    "\n",
    "    # convert y from {0, 1} to {-1, 1}\n",
    "    y = 2*y - 1\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = new_perceptron_data(n_points = 300, noise = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_new_perceptron_data(X, y, ax):\n",
    "    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n",
    "    targets = [-1, 1]\n",
    "    markers = [\"o\" , \",\"]\n",
    "    for i in range(2):\n",
    "        ix = y == targets[i]\n",
    "        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n",
    "    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new_perceptron_data is not linearly separable. Therefore, we expect the perceptron algorithm to not find a weight vector such that the loss\n",
    "is minimized to 0. The perceptron is run with this new_perceptron_data in the same way as before with the linearly separable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "X, y = new_perceptron_data()\n",
    "plot_new_perceptron_data(X, y, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a model and an optimizer\n",
    "p1 = Perceptron() \n",
    "opt1 = PerceptronOptimizer(p1)\n",
    "\n",
    "loss = 1.0\n",
    "\n",
    "max_iterations = 1000\n",
    "iterations = 0\n",
    "\n",
    "# for keeping track of loss values\n",
    "loss_vec = []\n",
    "\n",
    "n = X.size()[0]\n",
    "\n",
    "loss_threshold = 0.01\n",
    "\n",
    "#while loss > 0: # dangerous -- only terminates if data is linearly separable\n",
    "while iterations < max_iterations:\n",
    "    # not part of the update: just for tracking our progress \n",
    "\n",
    "    if (loss < loss_threshold):\n",
    "        break\n",
    "\n",
    "    loss = p1.loss(X, y) \n",
    "    loss_vec.append(loss)\n",
    "\n",
    "    # pick a random data point\n",
    "    i = torch.randint(n, size = (1,))\n",
    "    x_i = X[[i],:]\n",
    "    y_i = y[i]\n",
    "                \n",
    "    # perform a perceptron update using the random data point\n",
    "    opt1.step(x_i, y_i)\n",
    "\n",
    "    iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_vec, color = \"slategrey\")\n",
    "plt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\n",
    "labs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234567)\n",
    "\n",
    "# initialize a perceptron \n",
    "#p = Perceptron()\n",
    "#opt = PerceptronOptimizer(p)\n",
    "p1.loss(X, y)\n",
    "\n",
    "# set up the figure\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 5)\n",
    "fig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\n",
    "markers = [\"o\", \",\"]\n",
    "marker_map = {-1 : 0, 1 : 1}\n",
    "\n",
    "# initialize for main loop\n",
    "current_ax = 0\n",
    "loss = 1\n",
    "loss_vec = []\n",
    "\n",
    "while loss > 0:\n",
    "\n",
    "    ax = axarr.ravel()[current_ax]\n",
    "\n",
    "    # save the old value of w for plotting later\n",
    "    old_w = torch.clone(p1.w)\n",
    "\n",
    "    # make an optimization step -- this is where the update actually happens\n",
    "    # now p.w is the new value \n",
    "\n",
    "    #i, local_loss = opt.step(X, y)\n",
    "    i = torch.randint(n, size = (1,))\n",
    "    x_i = X[[i],:]\n",
    "    y_i = y[i]\n",
    "                \n",
    "    # perform a perceptron update using the random data point\n",
    "    local_loss = opt1.step(x_i, y_i)\n",
    "\n",
    "    # if a change was made, plot the old and new decision boundaries\n",
    "    # also add the new loss to loss_vec for plotting below\n",
    "    if local_loss > 0:\n",
    "        plot_new_perceptron_data(X, y, ax)\n",
    "        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n",
    "        loss = p1.loss(X, y).item()\n",
    "        loss_vec.append(loss)\n",
    "        draw_line(p1.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n",
    "        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n",
    "        # draw_line(w, -10, 10, ax, color = \"black\")\n",
    "        ax.set_title(f\"loss = {loss:.3f}\")\n",
    "        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n",
    "        current_ax += 1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEYOND 2 DIMENSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def five_dimension_perceptron_data(n_points = 300, noise = 0.2, p_dims = 5):\n",
    "    \n",
    "    y = torch.arange(n_points) >= int(n_points/2)\n",
    "    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n",
    "    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n",
    "\n",
    "    # convert y from {0, 1} to {-1, 1}\n",
    "    y = 2*y - 1\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = five_dimension_perceptron_data(n_points = 300, noise = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a model and an optimizer\n",
    "p3 = Perceptron() \n",
    "opt3 = PerceptronOptimizer(p3)\n",
    "\n",
    "loss = 1.0\n",
    "\n",
    "max_iterations = 100\n",
    "iterations = 0\n",
    "\n",
    "# for keeping track of loss values\n",
    "loss_vec3 = []\n",
    "\n",
    "n = X.size()[0]\n",
    "\n",
    "loss_threshold = 0.01\n",
    "\n",
    "#while loss > 0: # dangerous -- only terminates if data is linearly separable\n",
    "while iterations < max_iterations:\n",
    "    # not part of the update: just for tracking our progress \n",
    "\n",
    "    if (loss < loss_threshold):\n",
    "        break\n",
    "\n",
    "    loss = p3.loss(X, y) \n",
    "    loss_vec3.append(loss)\n",
    "\n",
    "    # pick a random data point\n",
    "    i = torch.randint(n, size = (1,))\n",
    "    x_i = X[[i],:]\n",
    "    y_i = y[i]\n",
    "                \n",
    "    # perform a perceptron update using the random data point\n",
    "    opt3.step(x_i, y_i)\n",
    "\n",
    "    iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_vec3, color = \"slategrey\")\n",
    "plt.scatter(torch.arange(len(loss_vec3)), loss_vec3, color = \"slategrey\")\n",
    "labs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the graph pictured above, the perceptron with > 2 dimensions converged, finding a weight vector such that the loss value is minimized to 0. I believe the data is linearly separable considering that in testing the data, the perceptron converges well before reaching the max iteration limit of 1000.\n",
    "\n",
    "The runtime complexity of the perceptron algorithm is O(n * d) where n is the number of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABSTRACT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('ml-0451')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "074ff0c55e5639ce65bc7082505f477fa89133c4941e973e162b1ebaf9c4e0b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
