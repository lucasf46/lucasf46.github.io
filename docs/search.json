[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a test."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/BlogPost4/BlogPost4-4.html",
    "href": "posts/BlogPost4/BlogPost4-4.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "LINK TO LOGISTIC REGRESSION ON GITHUB:\nhttps://github.com/lucasf46/lucasf46.github.io/blob/main/posts/BlogPost4/logistic.py\n\n\nINTRODUCTION:\nThe purpose of this blog post is to implement Logisitc Regression and to understand how the algorithm can be optimized by adjusting parameters such as beta (the momentum factor) to speed up convergence. In this post, we will look at Vanilla Gradient descent where the momentum factor is set to 0, Spicy Gradient Descent which incorporates momentum, as well as overfitting data where the number of dimensions is greater than the number of points.\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\nTo test our implementation of Logistic Regression, we can borrow a few functions from the Perceptron blog post to create and plot some classification data.\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(n_points = 500, noise = 0.5, p_dims = 2)\n\n\ndef plot_classification_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \"^\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\nfig, ax = plt.subplots(1, 1)\nX, y = classification_data()\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\nHere, we can visualize the data the Logistic Regression will be working with. The data can be manipulated by adjusting the noise parameter to increase the difficulty of the classification problem.\n\n\nExperiment 1: Vanilla Gradient Descent\nFirst, we initialize our model.\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nHere is the gradient descent loop, where the classification data has two dimensions, Alpha is sufficiently small, and Beta = 0.\n\nloss_vec = []\nfor _ in range(2000):\n    loss = opt.step(X, y, alpha = 0.1, beta = 0)\n    loss_vec.append(loss)\n\n\nfig, ax = plt.subplots(1, 1)\nopt_loss = loss_vec[len(loss_vec) - 1]\nax.grid(True)\nax.set_title(f\"Logistic Loss Value= {opt_loss:.3f}\")\nax.set(xlim = (-0.5, 1.5), ylim = (-0.5, 1.5))\n\nplot_classification_data(X, y, ax)\ndraw_line(torch.clone(LR.w), -2, 2, ax)\n\n\n\n\n\n\n\n\nHere, we can plot the decision boundary after 2000 iterations, which yields a loss value close to 0. The low loss value indicates our implementation of Logistic Regression is working.\n\nplt.plot(loss_vec)\nlabs = plt.gca().set(xlabel = \"Number of Iterations\", ylabel = \"Loss Value\", title = \"Experiment 1: Vanilla Gradient Descent\")\n\n\n\n\n\n\n\n\nHere, we graph loss over the iterations where we see the loss decreases monotonically, further indicating the implementation of Part A is correct.\n\n\nExperiment 2: Benefits of Momentum\nAgain, we initialize our model.\n\nMLR = LogisticRegression() \nMopt = GradientDescentOptimizer(MLR)\n\nThis time, we adjust beta value to observe the effects of gradient descent with momentum, instead of vanilla gradient descent (beta = 0). We also need to adjust our alpha value slightly to ensure we can see the effects of the momentum graphically.\n\nMloss_vec = []\nfor _ in range(2000):\n    loss = Mopt.step(X, y, alpha = 0.2, beta = 0.9)\n    Mloss_vec.append(loss)\n\n\nfig, ax = plt.subplots(1, 1)\nopt_loss = Mloss_vec[len(Mloss_vec) - 1]\nax.grid(True)\nax.set_title(f\"Logistic Loss Value= {opt_loss:.3f}\")\nax.set(xlim = (-0.5, 1.5), ylim = (-0.5, 1.5))\n\nplot_classification_data(X, y, ax)\ndraw_line(torch.clone(MLR.w), -2, 2, ax)\n\n\n\n\n\n\n\n\nHere, we can see the Logistic Regression is working. A decision boundary clearly delineates the two groups of data with a loss value close to 0.\n\nplt.plot(Mloss_vec, color = \"red\", label = \"Spicy\")\nplt.plot(loss_vec, color = \"tan\", label = \"Vanilla\")\nplt.legend()\nlabs = plt.gca().set(xlabel = \"Number of Iterations\", ylabel = \"Loss Value\", title = \"Experiment 2: Gradient Descent with Momentum\")\n\n\n\n\n\n\n\n\nIn the graph above, Gradient descent with momentum (Spicy converges to the correct weight vector faster than gradient descent without (Vanilla). Alpha was modified to 0.15 for Spicy, compared to 0.1 for Vanilla, in order to clearly show how Spicy Gradient Descent’s loss value decreases quicker and converges sooner.\n\n\nExperiment 3: Overfitting\nHelper accuracy function that calculates the accuracy of the LR model on the Overfitting data.\n\ndef calc_accuracy(model, X, y):\n    accuracy = torch.mean((1.0 * (model.predict(X) == y)))\n    return accuracy\n\nGenerating Train and Test datasets:\n\nX_train, y_train = classification_data(n_points = 50, noise = 0.2, p_dims = 100)\n\nX_test, y_test = classification_data(n_points = 50, noise = 0.2, p_dims = 100)\n\n\nOLR = LogisticRegression() \nOopt = GradientDescentOptimizer(OLR)\n\n\nOloss_vec = []\nTrain_accuracy = []\nTest_accuracy = []\n\nfor _ in range(2000):\n    loss = Oopt.step(X_train, y_train, alpha = 0.2, beta = 0.9)\n    Train_accuracy.append((1.0 * (OLR.predict(X_train) == y_train)).mean())\n    Test_accuracy.append((1.0 * (OLR.predict(X_test) == y_test)).mean())\n    Oloss_vec.append(loss)\n\nSimilar to keeping track of the loss value in the loss_vec, we store the accuracies of each iteration in arrays to then be graphed later.\n\nplt.plot(Train_accuracy, color = \"red\", label = \"Train\")\nplt.plot(Test_accuracy, color = \"blue\", label = \"Test\")\nplt.legend()\nlabs = plt.gca().set(xlabel = \"Number of Iterations\", ylabel = \"Accuracy (%)\", title = \"Experiment 3: Overfitting\")\n\n\n\n\n\n\n\n\nHere we graph model accuracy, as a percentage, against the number of iterations. The experiment is entitled “Overfitting” so it is logical that the accuracy of the model test data is less than the 100% accuracy of the model on the training data, as illustrated above.\n\n\nDISCUSSION"
  },
  {
    "objectID": "posts/goal-setting.html",
    "href": "posts/goal-setting.html",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Lucas Flemming\n\n\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI am particularly interested in the Theory and Implementation. I would like to develop a strong foundation of exactly what is going on behind the scenes because until this course, Machine Learning has been somewhat of a mystery to me. I’m trying to not only understand Machine Learning concepts but apply them effectively in formats such as the blog posts but more importantly, in the final project which could incorporate real world data that carries significance to me (no offense to the Penguins on Dream Island).\n\n\n\n\n\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nI plan on seriously attempting every blog post as a way to familiarize myself with the various aspects of ML. I feel as though 3/4 of the blog posts is an ambitious, but achievable goal for me. It is ambitious especially because I’m on the baseball team and I’m in season so much of time is taken up in practices and games.\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nI think completing every warmup to the best of my ability is the standard, so I expect nothing less. Even if I struggle with a warm-up activity, I will make an honest effort and communicate my challenges to group members, that’s what they are there for.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nI have a former teammate who is running a startup focused on connecting researchers with grants called ATOM, so I feel like there could be an opportunity to integrate machine learning with that process. I would have to talk to him to see where ML would be most useful. I would communicate with my team members to create a loose schedule that could be adjusted to ensure the workload is shared. I’m also interested in doing something sports related because I’m passionate about professional/collegiate athletics and the project would feel less like work. I hope to use this project as something I can build upon in my time after Middlebury as I continue to look for future employment."
  },
  {
    "objectID": "posts/goal-setting.html#what-youll-learn",
    "href": "posts/goal-setting.html#what-youll-learn",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "The knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI am particularly interested in the Theory and Implementation. I would like to develop a strong foundation of exactly what is going on behind the scenes because until this course, Machine Learning has been somewhat of a mystery to me. I’m trying to not only understand Machine Learning concepts but apply them effectively in formats such as the blog posts but more importantly, in the final project which could incorporate real world data that carries significance to me (no offense to the Penguins on Dream Island)."
  },
  {
    "objectID": "posts/goal-setting.html#what-youll-achieve",
    "href": "posts/goal-setting.html#what-youll-achieve",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Most blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nI plan on seriously attempting every blog post as a way to familiarize myself with the various aspects of ML. I feel as though 3/4 of the blog posts is an ambitious, but achievable goal for me. It is ambitious especially because I’m on the baseball team and I’m in season so much of time is taken up in practices and games.\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nI think completing every warmup to the best of my ability is the standard, so I expect nothing less. Even if I struggle with a warm-up activity, I will make an honest effort and communicate my challenges to group members, that’s what they are there for.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nI have a former teammate who is running a startup focused on connecting researchers with grants called ATOM, so I feel like there could be an opportunity to integrate machine learning with that process. I would have to talk to him to see where ML would be most useful. I would communicate with my team members to create a loose schedule that could be adjusted to ensure the workload is shared. I’m also interested in doing something sports related because I’m passionate about professional/collegiate athletics and the project would feel less like work. I hope to use this project as something I can build upon in my time after Middlebury as I continue to look for future employment."
  },
  {
    "objectID": "posts/BlogPost1/BlogPost1-1.html",
    "href": "posts/BlogPost1/BlogPost1-1.html",
    "title": "Palmer Penguins Blog Post",
    "section": "",
    "text": "ABSTRACT\nThis blog post seeks to build a model for predicting the species of a given penguin based on features of that penguin, such as flipper length, body mass, or the island they inhabit. Constructing the model is part of a pipeline that begins with data preparation where we remove the target variable, and ensure the columns of the data frame are suitable for model fitting. As part of fitting the model to both the training and test data, cross-validation helps reduce the possibility of the model over fitting to the training data, resulting in poorer performance on the testing data. We can then evaluate and visualize the performance of our model (in this case, we are shooting for 100%) by plotting decision boundaries or by looking at a confusion matrix to affirm our results.\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\nDATA PREPARATION\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis = 1)\n    df = pd.get_dummies(df, dtype = float)\n    return df, y\n\n\nX_train, y_train = prepare_data(train)\n\nX_train\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\n51.1\n16.5\n225.0\n5250.0\n8.20660\n-26.36863\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n271\n35.9\n16.6\n190.0\n3050.0\n8.47781\n-26.07821\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n272\n39.5\n17.8\n188.0\n3300.0\n9.66523\n-25.06020\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n\n\n273\n36.7\n19.3\n193.0\n3450.0\n8.76651\n-25.32426\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n\n\n274\n42.4\n17.3\n181.0\n3600.0\n9.35138\n-24.68790\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n\n\n\n\n256 rows × 14 columns\n\n\n\n\n\nEXPLORE:\nCreate two visualizations of the data and one summary table.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/palmer-penguins.csv\"\ndf = pd.read_csv(url)\n\nSumTable = df.groupby(['Species', 'Sex']).aggregate({'Body Mass (g)': 'mean'})\nSumTable2 = df.groupby(['Species', 'Island']).aggregate({'Culmen Length (mm)': 'mean'})\n\n\nSumTable\n\n\n\n\n\n\n\n\n\nBody Mass (g)\n\n\nSpecies\nSex\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\nFEMALE\n3368.835616\n\n\nMALE\n4043.493151\n\n\nChinstrap penguin (Pygoscelis antarctica)\nFEMALE\n3527.205882\n\n\nMALE\n3938.970588\n\n\nGentoo penguin (Pygoscelis papua)\n.\n4875.000000\n\n\nFEMALE\n4679.741379\n\n\nMALE\n5484.836066\n\n\n\n\n\n\n\nIn this summary table, we can see the disparities in body mass between males and females of the same species, as well as between species. Take a look at the female Gentoo penguins. On average, they weigh roughly 1000 grams more than the average female Adele or Chinstrap, signifying penguin Sex might be a favorable feature to include when constructing a model.\n\nSumTable2\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\n\n\nSpecies\nIsland\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\nBiscoe\n38.975000\n\n\nDream\n38.501786\n\n\nTorgersen\n38.950980\n\n\nChinstrap penguin (Pygoscelis antarctica)\nDream\n48.833824\n\n\nGentoo penguin (Pygoscelis papua)\nBiscoe\n47.504878\n\n\n\n\n\n\n\nThis summary table examines the average culmen length of penguin species on each island. Looking closely, we see that only Adele and Gentoo inhabit Biscoe island and the difference in their average culmen lengths is roughly 9 millimeters. Similarly, only Adelie and Chinstrap live on Dream island, and the average difference in culmen length there is roughly 10 millimeters, a significant amount.\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n# Plot first scatterplot\nsns.scatterplot(data=df, x='Culmen Length (mm)', y='Culmen Depth (mm)', hue= 'Species', ax=axes[0])\naxes[0].set_title('Culmen Length vs. Culmen Depth by Species')\n\n# Plot second scatterplot\n# Filter to get penguins living on Dream Island\nIsland_Dream_df = df[df['Island'] == 'Dream']\nsns.scatterplot(x='Culmen Length (mm)', y='Culmen Depth (mm)', data= Island_Dream_df, hue= 'Species', ax=axes[1])\naxes[1].set_title('Culmen Length vs. Culmen Depth on Dream Island')\n\nplt.show()\n\n\n\n\n\n\n\n\nThe scatterplot on the left graphs culmen depth vs. culmen length which yields three fairly distinct regions characterized by penguin species, suggesting these two features would be suitable for a model.\nThe scatterplot on the right visualizes a trend shown by the summary table which indicates that only Adelie and Chinstrap inhabit Dream Island and the two species are relatively distinguishable by the ratio of the ratio of their culmen length and culmen depth.\n\n\nFEATURE SELECTION:\nThe block of code below implements two models: Logistic Regression and Decision Tree Classifier. Iterating through the combinations of the columns, we fit the Logistic Regression model to each combination, and utilize GridSearchCV, a tool for cross-validation that works in junction with the DCF model to find the best parameters for the DCF model (depth). We can then filter the results to find the combinations of features that yield an accuracy of 1.0 from either the Logistic Regression or DCF model.\n\nfrom itertools import combinations\nimport numpy as np\n\n# Alternative tools to select features\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\n# Types of models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Optimizing max_depth\nfrom sklearn.model_selection import GridSearchCV\n\n# Cross validation\nfrom sklearn.model_selection import cross_val_score\n\n# Qualitative and quantitative columns I've selected\nall_qual_columns = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Stage_Adult, 1 Egg Stage\",\"Clutch Completion_No\", \"Clutch Completion_Yes\", \"Sex_FEMALE\", \"Sex_MALE\" ]\nall_quant_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\nLR = LogisticRegression(max_iter = 1000)  \n\nclf = DecisionTreeClassifier(random_state=0)\nparameters = {'max_depth': np.arange(1, 21)}\n\nall_cols = []\n\nfor qual in all_qual_columns: \n    qual_cols = [col for col in X_train.columns if qual in col] \n    for pair in combinations(all_quant_columns, 2):\n        cols =  list(pair) + qual_cols \n\n        # Linear Regression Model\n\n        LR.fit(X_train[cols], y_train)\n        LR_accuracy = LR.score(X_train[cols], y_train)\n        # Cross validation step \n        cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n\n        # Decision Tree Classifier Model\n\n        # Grid search with cross-validation to find the best parameters for the classifier\n        grid_search = GridSearchCV(clf, parameters, cv = 5)\n        # Fit grid search to training data\n        grid_search.fit(X_train[cols], y_train)\n        # Gets best estimator (classifier) from the grid search\n        best_clf = grid_search.best_estimator_\n        best_params = grid_search.best_params_\n        # Accuracy of best estimator (classifier) on the training data\n        dt_accuracy = best_clf.score(X_train[cols], y_train)\n        # Cross validation step\n        cv_scores_DecisionTree = cross_val_score(best_clf, X_train[cols], y_train, cv = 5)\n        \n        # print only the combination of features that produce 100% accuracy\n        if (dt_accuracy == 1.0 or LR_accuracy == 1.0):\n            print(f\"Features: {cols}, LR Accuracy: {LR_accuracy}, DT Accuracy: {dt_accuracy}, Best Params: {best_params}\")\n        \n\nFeatures: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Dream'], LR Accuracy: 0.99609375, DT Accuracy: 1.0, Best Params: {'max_depth': 5}\nFeatures: ['Body Mass (g)', 'Delta 13 C (o/oo)', 'Island_Dream'], LR Accuracy: 0.82421875, DT Accuracy: 1.0, Best Params: {'max_depth': 8}\nFeatures: ['Culmen Length (mm)', 'Body Mass (g)', 'Sex_FEMALE'], LR Accuracy: 0.97265625, DT Accuracy: 1.0, Best Params: {'max_depth': 7}\nFeatures: ['Culmen Length (mm)', 'Body Mass (g)', 'Sex_MALE'], LR Accuracy: 0.97265625, DT Accuracy: 1.0, Best Params: {'max_depth': 7}\n\n\nWe now have multiple combinations of features we can choose from. As we can see, none of the Logistic Regression models yielded a perfect accuracy of 1.0, but came quite close.\n\n\nTESTING\nTaking one of the feature combinations generated by the Decision Tree Classifier that yielded 100% accuracy and using it as the features for the model on the testing data.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest_clf = DecisionTreeClassifier(random_state=0, max_depth= 7)\nX_test, y_test = prepare_data(test)   \n\ntest_cols = ['Culmen Length (mm)', 'Body Mass (g)', 'Sex_MALE', 'Sex_FEMALE']\n\ntest_clf = test_clf.fit(X_test[test_cols], y_test)\n\nTest_score = test_clf.score(X_test[test_cols], y_test)\n\n\nX_test[test_cols]\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nBody Mass (g)\nSex_MALE\nSex_FEMALE\n\n\n\n\n0\n41.7\n4700.0\n0.0\n1.0\n\n\n1\n50.7\n4050.0\n1.0\n0.0\n\n\n2\n38.1\n3425.0\n0.0\n1.0\n\n\n3\n39.6\n3550.0\n0.0\n1.0\n\n\n4\n43.3\n4400.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n\n\n64\n42.5\n3350.0\n0.0\n1.0\n\n\n65\n54.3\n5650.0\n1.0\n0.0\n\n\n66\n59.6\n6050.0\n1.0\n0.0\n\n\n67\n36.9\n3500.0\n0.0\n1.0\n\n\n68\n49.6\n4750.0\n1.0\n0.0\n\n\n\n\n68 rows × 4 columns\n\n\n\n\n\nPLOTTING DECISION REGIONS\nUsing code from the Palmer’s Penguins blog post, and corresponding lectures, we can plot the decision regions generated from our model.\n\nfrom matplotlib.patches import Patch\n\n# plot_regions function borrowed from Palmer's Penguins blog post instructions / lecture\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches)\n      \n      plt.tight_layout()\n\nWe can now plot the decision regions to visualize where the model classifies points in the data.\n\nplot_regions(test_clf, X_test[test_cols], y_test)\n\n\n\n\n\n\n\n\n\n\nCONFUSION MATRIX\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = test_clf.predict(X_test[test_cols])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nThe confusion matrix demonstrates the model correctly identified the members of each penguin species with 100% accuracy.\n\n\nDISCUSSION\nIn summary, the Decision Tree Classifier model (DTC) using culmen length, body mass, and penguin sex produced 100% accuracy on both the testing and the training data. It is important to recognize, however, that the fitting the model is only one step in the process. Data preparation and safe training practices (cross-validation) are fundamental pieces that work in junction to the fitting of the model on the data. Visualizing the model’s outcomes is also an important part of the pipeline as a way to show yourself and others what the classification of the data looks like."
  },
  {
    "objectID": "posts/BlogPost3/BlogPost3-3.html",
    "href": "posts/BlogPost3/BlogPost3-3.html",
    "title": "Perceptron Blog Post",
    "section": "",
    "text": "https://github.com/lucasf46/lucasf46.github.io/blob/main/posts/BlogPost3/perceptron.py"
  },
  {
    "objectID": "posts/BlogPost3/BlogPost3-3.html#check-implementation",
    "href": "posts/BlogPost3/BlogPost3-3.html#check-implementation",
    "title": "Perceptron Blog Post",
    "section": "Check Implementation:",
    "text": "Check Implementation:\n\n# instantiate a model and an optimizer\ntestp = Perceptron() \ntestopt = PerceptronOptimizer(testp)\n\ntestloss = 1.0\n\n# for keeping track of loss values\ntestloss_vec = []\n\ntestn = X.size()[0]\n\n#while loss &gt; 0: # dangerous -- only terminates if data is linearly separable\nwhile testloss &gt; 0:\n\n    testloss = testp.loss(X, y) \n    testloss_vec.append(testloss)\n\n    # pick a random data point\n    testi = torch.randint(testn, size = (1,))\n    testx_i = X[[testi],:]\n    testy_i = y[testi]\n                \n    # perform a perceptron update using the random data point\n    testopt.step(testx_i, testy_i)\n\nprint(\"Perceptron loss: \", testloss)\n\nPerceptron loss:  tensor(0.)\n\n\nAt the moment, without any further experimentation or testing, our implementation of the Perceptron is “probably” working because our calculated loss = 0."
  },
  {
    "objectID": "posts/BlogPost3/BlogPost3-3.html#experiment-1-linearly-separable-data",
    "href": "posts/BlogPost3/BlogPost3-3.html#experiment-1-linearly-separable-data",
    "title": "Perceptron Blog Post",
    "section": "Experiment 1: Linearly Separable Data",
    "text": "Experiment 1: Linearly Separable Data\nOur first experiment involves the data shown earlier. It is linearly separable so we expect that our algorithm will converge with the loss value = 0.\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\n\n# instantiate a model and an optimizer\ntorch.manual_seed(1234567)\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# Initializing iteration counts\nmax_iterations = 100\niterations = 0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nloss_threshold = 0.001\n\n#while loss &gt; 0: # dangerous -- only terminates if data is linearly separable\nwhile iterations &lt; max_iterations:\n    # not part of the update: just for tracking our progress \n\n    # if loss is close enough to 0, given a certain threshold, treat it as though the perceptron converged to exactly 0\n    if (loss &lt; loss_threshold):\n        break\n\n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n                \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n    # increment iterations\n    iterations += 1\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\", title = \"Experiment 1: Perceptron Loss\")\n\n\n\n\n\n\n\n\nHere, we can visualize the Perceptrons accuracy for each iteration as it ultimately converges on a loss of 0.\n\n# initialize a perceptron \np1 = Perceptron()\nopt1 = PerceptronOptimizer(p1)\np1.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p1.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n                \n    # perform a perceptron update using the random data point\n    local_loss = opt1.step(x_i, y_i)\n\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p1.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p1.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\nHere, we can visualize the Perceptron finding a decision boundary that fully separates the data. The solid black line represents the current decision boundary while the dotted black line represents the previous decision boundary."
  },
  {
    "objectID": "posts/BlogPost3/BlogPost3-3.html#experiment-2-non-linearly-separable-data",
    "href": "posts/BlogPost3/BlogPost3-3.html#experiment-2-non-linearly-separable-data",
    "title": "Perceptron Blog Post",
    "section": "Experiment 2: Non-linearly Separable Data",
    "text": "Experiment 2: Non-linearly Separable Data\nWe can then experiment with how the Perceptron algorithm runs on data that is NOT linearly separable. To produce non-linearly separable data, we just need to modify the percpetron_data and plot_perceptron_data function by adjusting the noise parameter.\n\ndef new_perceptron_data(n_points = 300, noise = 0.5, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\n\nX, y = new_perceptron_data(n_points = 300, noise = 0.5)\n\n\ndef plot_new_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n\nfig, ax = plt.subplots(1, 1)\nX, y = new_perceptron_data()\nplot_new_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nThis new_perceptron_data is not linearly separable. Therefore, we expect the perceptron algorithm to not find a weight vector such that the loss is minimized to 0. The perceptron is run with this new_perceptron_data in the same way as before with the linearly separable data.\n\n# instantiate a model and an optimizer\np1 = Perceptron() \nopt1 = PerceptronOptimizer(p1)\n\nloss = 1.0\n\nmax_iterations = 1000\niterations = 0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nloss_threshold = 0.01\n\n#while loss &gt; 0: # dangerous -- only terminates if data is linearly separable\nwhile iterations &lt; max_iterations:\n    # not part of the update: just for tracking our progress \n\n    if (loss &lt; loss_threshold):\n        break\n\n    loss = p1.loss(X, y) \n    loss_vec.append(loss)\n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n                \n    # perform a perceptron update using the random data point\n    opt1.step(x_i, y_i)\n\n    iterations += 1\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\", title = \"Experiment 2: Perceptron Loss\")\n\n\n\n\n\n\n\n\nHere, we can see that unlike the linearly separable plot, the perceptron runs until the max number of iterations is reached without converging on a loss of 0 and achieving perfect accuracy.\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = []\n\niterations = 0\n\nwhile iterations &lt; 1000:\n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n                \n    # perform a perceptron update using the random data point\n    local_loss = opt.step(x_i, y_i)\n\n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n    iterations += 1\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_new_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")      \n       \n\n\n\n\n\n\n\n\nThis is the decision boundary after 1000 iterations of the algorithm on the non-linearly separable data. It is close, but the loss is still above 0 at 0.0733\n\ntorch.manual_seed(12345)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = []\n\niterations = 0\n\nwhile iterations &lt; 2000:\n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n                \n    # perform a perceptron update using the random data point\n    local_loss = opt.step(x_i, y_i)\n\n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n    iterations += 1\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_new_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")   \n\n\n\n\n\n\n\n\nThis is the decision boundary after another 2000 iterations of the algorithm on the non-linearly separable data. It is closer, but the loss is still above 0 at 0.0533"
  },
  {
    "objectID": "posts/BlogPost3/BlogPost3-3.html#experiment-3-beyond-2-dimensions",
    "href": "posts/BlogPost3/BlogPost3-3.html#experiment-3-beyond-2-dimensions",
    "title": "Perceptron Blog Post",
    "section": "Experiment 3: Beyond 2 Dimensions",
    "text": "Experiment 3: Beyond 2 Dimensions\nWe can generate perceptron data with 5 features (dimensions) by adjusting the p_dims argument in the perceptron_data function we’ve been using.\n\ndef five_dimension_perceptron_data(n_points = 300, noise = 0.2, p_dims = 5):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\n\nX, y = five_dimension_perceptron_data(n_points = 300, noise = 0.2)\n\n\n# instantiate a model and an optimizer\ntorch.manual_seed(1234567)\np3 = Perceptron() \nopt3 = PerceptronOptimizer(p3)\n\nloss = 1.0\n\nmax_iterations = 100\niterations = 0\n\n# for keeping track of loss values\nloss_vec3 = []\n\nn = X.size()[0]\n\nloss_threshold = 0.001\n\n#while loss &gt; 0: # dangerous -- only terminates if data is linearly separable\nwhile iterations &lt; max_iterations:\n    # not part of the update: just for tracking our progress \n\n    if (loss &lt; loss_threshold):\n        break\n\n    loss = p3.loss(X, y) \n    loss_vec3.append(loss)\n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n                \n    # perform a perceptron update using the random data point\n    opt3.step(x_i, y_i)\n\n    iterations += 1\nprint(\"Perceptron Loss: \", loss)\n\nPerceptron Loss:  tensor(0.)\n\n\nWe can see the running the perceptron on the five-dimensional data yields a loss of 0, indicating the data is also linearly separable. Let’s look at the visualization of the loss value after each iteration to confirm.\n\nplt.plot(loss_vec3, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec3)), loss_vec3, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\", title = \"Experiment 3: Perceptron Loss\")\n\n\n\n\n\n\n\n\nBased on the graph, the perceptron on data with 3+ dimensions converged, finding a weight vector such that the loss value is minimized to 0, therefore the data is linearly separable.\nTo analyze the runtime of the perceptron algorithm, lets look at the pseudocode provided in the notes:\np = number of features\n\nStart with random weight vector W(0) that has length p\nPick a random point i in X, also has length p\nCompute dot product of point i and W(0)\nIf dot product &lt; 0, perform update\n    W(t + 1) = W(t) + (yi * xi)\nFor each iteration of our Perceptron update, we compute a dot product of two vectors, each with length p equal to the amount of features. In computing this dot product, we preform p multiplications (element wise multiplication) and p-1 additions (summation). If we assume each of these operations to be time constant operations, the time complexity of one step is 2p-1 or O(p)."
  },
  {
    "objectID": "posts/BlogPost2/BlogPost2-2.html",
    "href": "posts/BlogPost2/BlogPost2-2.html",
    "title": "Optimal Decision Making Blog Post",
    "section": "",
    "text": "In this blog post, we’re addressing two aspects of the credit lending decision process. Firstly, we seek to find a score function and a threshold. which help to maximize the total expected profit for a hypothetical bank from their lending decisions. We’re exploring various features in the dataset. Secondly, we examine how credit lending affects different groups of prospective borrowers, in the process shedding light on patterns or biases that emerge between the various groups asking for loans from the bank."
  },
  {
    "objectID": "posts/BlogPost2/BlogPost2-2.html#summary-tables",
    "href": "posts/BlogPost2/BlogPost2-2.html#summary-tables",
    "title": "Optimal Decision Making Blog Post",
    "section": "Summary Tables:",
    "text": "Summary Tables:\n\n# Income bins\nbins = [0, 50000, 100000, 200000, float('inf')]  \n\n# Labels for bins\nlabels = ['0-50k', '50k-100k', '100k-200k', '200k+']\n\n# New column in data frame: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html\ndf_train['income_bins'] = pd.cut(df_train['person_income'], bins=bins, labels=labels, right=False)\n\n# Group by the income bins and calculate the mean loan amount for each category\nsummary_table = df_train.groupby('income_bins', observed=True).agg({'loan_amnt': 'mean', 'cb_person_cred_hist_length': 'mean'}).reset_index()\nsummary_table.columns = [\"Income Bins\", \"Mean Loan Amnt\", \"Mean Credit History Length\"]\n\nsummary_table2 = df_train.groupby('person_home_ownership').agg({'loan_amnt': 'mean', 'loan_int_rate': 'mean', 'cb_person_cred_hist_length': 'mean' }).reset_index()\nsummary_table2.columns = [\"Person Home Ownership\", \"Mean Loan Amount\", \"Mean Interest Rate\", \"Mean Credit History Length\"]\n\n\nsummary_table\n\n\n\n\n\n\n\n\nIncome Bins\nMean Loan Amnt\nMean Credit History Length\n\n\n\n\n0\n0-50k\n7026.580284\n5.551688\n\n\n1\n50k-100k\n10464.931791\n5.777665\n\n\n2\n100k-200k\n13681.890377\n6.323887\n\n\n3\n200k+\n17331.845238\n8.364286\n\n\n\n\n\n\n\nThe summary table above demonstrates how a person’s income level affects the loan amount a borrower has access to as well as the average length of credit a borrower is offered.\n\nsummary_table2\n\n\n\n\n\n\n\n\nPerson Home Ownership\nMean Loan Amount\nMean Interest Rate\nMean Credit History Length\n\n\n\n\n0\nMORTGAGE\n10562.137462\n10.491245\n5.930430\n\n\n1\nOTHER\n11235.795455\n12.059221\n5.465909\n\n\n2\nOWN\n8978.912626\n10.850169\n5.834854\n\n\n3\nRENT\n8843.507973\n11.448571\n5.679575\n\n\n\n\n\n\n\nThe summary table above shows how a person’s home ownership relates to the amount of money a borrower asks for as well as the interest rate they are offered."
  },
  {
    "objectID": "posts/BlogPost2/BlogPost2-2.html#data-visualizations",
    "href": "posts/BlogPost2/BlogPost2-2.html#data-visualizations",
    "title": "Optimal Decision Making Blog Post",
    "section": "Data Visualizations",
    "text": "Data Visualizations\n\n# Graph setup\nplt.figure(figsize=(12, 20))\n\n# Modify dataset to remove datapoints with person_age &gt; 90 and person_employment_length &gt; 40\nmod_df_train = df_train[df_train['person_age'] &lt; 40]\nmod_df_train = mod_df_train[mod_df_train['person_emp_length'] &lt; 40]\n\nplt.subplot(3, 1, 1, title=\"Box Plot: Age vs. Loan Intent\")\nsns.boxplot(x= \"loan_intent\", y= \"person_age\", data= mod_df_train)\n\n#plt.subplot(3, 1, 2, title=\"Scatter Plot: Interest Rate vs. Employment Length\")\n#sns.scatterplot(x= 'person_home_ownership', y= 'person_income', hue= 'cb_person_default_on_file', data= df_train)\n#sns.barplot(x= 'person_home_ownership', y= 'person_income', hue= 'cb_person_default_on_file', data= df_train)\n\n#Medical_Education_Venture = [\"MEDICAL\", \"EDUCATION\", \"VENTURE\"]\nMedical_Education_Venture = [\"MEDICAL\", \"EDUCATION\"]\n\nmod_df_train = mod_df_train[mod_df_train['person_income'] &lt; 60000]\nfiltered_df = mod_df_train[mod_df_train['loan_intent'].isin(Medical_Education_Venture)]\n\n\n\nplt.subplot(3, 1, 2, title=\"Scatter Plot: Interest Rate vs. Person Income\")\n#sns.scatterplot(x= 'person_income', y= 'loan_int_rate', hue= 'cb_person_default_on_file', style= 'loan_intent' , data= df_train)\nsns.scatterplot(x= 'person_income', y= 'loan_int_rate', style= 'cb_person_default_on_file', hue= 'loan_intent' , data= filtered_df)\n\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/BlogPost2/BlogPost2-2.html#is-it-more-difficult-for-people-in-certain-age-groups-to-access-credit-under-your-proposed-system",
    "href": "posts/BlogPost2/BlogPost2-2.html#is-it-more-difficult-for-people-in-certain-age-groups-to-access-credit-under-your-proposed-system",
    "title": "Optimal Decision Making Blog Post",
    "section": "Is it more difficult for people in certain age groups to access credit under your proposed system?",
    "text": "Is it more difficult for people in certain age groups to access credit under your proposed system?\n\n# Income bins\nbins = [0, 20, 30, 40, 50, 60, 70, 80, float('inf')]  \n\n# Labels for bins\nlabels = ['0-20', '20-30', '30-40', '40-50', '50-60', '60-70', '70-80', '80+']\n\n# New column in data frame: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html\nX_test['age_bins'] = pd.cut(X_test['person_age'], bins=bins, labels=labels, right=False)\n\n# Group by the income bins and calculate the mean loan amount for each category\nsummary_table3 = X_test.groupby('age_bins', observed=True).agg({'Default Unlikely': 'mean', 'Correct Assumption': 'mean', 'Total Profit': 'mean'}).reset_index()\nsummary_table3.columns = [\"Age Bins\", \"Avg Prob of Repay (at Training Optimal Threshold)\", \"Avg Bank Assumption\", \"Avg Total Profit\"]\n\nsummary_table3\n\n\n\n\n\n\n\n\nAge Bins\nAvg Prob of Repay (at Training Optimal Threshold)\nAvg Bank Assumption\nAvg Total Profit\n\n\n\n\n0\n20-30\n0.868872\n0.175803\n5455.469283\n\n\n1\n30-40\n0.890895\n0.165620\n5930.292986\n\n\n2\n40-50\n0.903846\n0.150000\n5623.639173\n\n\n3\n50-60\n0.904762\n0.166667\n5831.695471\n\n\n4\n60-70\n0.750000\n0.250000\n9409.626847\n\n\n5\n70-80\n1.000000\n0.500000\n2957.459942\n\n\n\n\n\n\n\nAfter grouping the borrowers into age brackets, we can see that a borrower in the youngest age bracket (20 to 30 years old) is least likely to repay a loan offered by the bank."
  },
  {
    "objectID": "posts/BlogPost2/BlogPost2-2.html#is-it-more-difficult-for-people-to-get-loans-in-order-to-pay-for-medical-expenses-how-does-this-compare-with-the-actual-rate-of-default-in-that-group",
    "href": "posts/BlogPost2/BlogPost2-2.html#is-it-more-difficult-for-people-to-get-loans-in-order-to-pay-for-medical-expenses-how-does-this-compare-with-the-actual-rate-of-default-in-that-group",
    "title": "Optimal Decision Making Blog Post",
    "section": "Is it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that group?",
    "text": "Is it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that group?\n\n# Create a data frame with the individuals with medical loan intent\nMedical_analysis = X_test[X_test[\"loan_intent_MEDICAL\"]]\n\n# Filtering for people who were given loans, because the probability of not defaulting was above a certain threshold\nPredDefaultMedical = Medical_analysis[Medical_analysis[\"Default Unlikely\"] == 0]\nActualDefaultMedical = PredDefaultMedical[PredDefaultMedical[\"Correct Assumption\"] == 1]\n\n# Filter to find average predicted probability\nAvgMedicalProb = Medical_analysis.agg({\"Default Prob\" : \"mean\"})\n\n# Generate summary table to show number of borrowers with \nsummary_table4 = PredDefaultMedical.groupby(\"Correct Assumption\").agg({'Default Unlikely': 'count'})\n\n\nAvgMedicalProb\n\nDefault Prob    0.746771\ndtype: float64\n\n\n\nsummary_table4\n\n\n\n\n\n\n\n\nDefault Unlikely\n\n\nCorrect Assumption\n\n\n\n\n\n0\n130\n\n\n1\n27\n\n\n\n\n\n\n\nAdd explanation here:\n\nX_test.columns\n\nIndex(['person_age', 'person_income', 'person_emp_length', 'loan_amnt',\n       'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length',\n       'person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER',\n       'person_home_ownership_OWN', 'person_home_ownership_RENT',\n       'loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION',\n       'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL',\n       'loan_intent_PERSONAL', 'loan_intent_VENTURE',\n       'cb_person_default_on_file_N', 'cb_person_default_on_file_Y',\n       'Default Prob', 'Repay Profit', 'Default Profit', 'Default Unlikely',\n       'Correct Assumption', 'Total Profit', 'age_bins'],\n      dtype='object')"
  },
  {
    "objectID": "posts/BlogPost2/BlogPost2-2.html#what-about-people-seeking-loans-for-business-ventures-or-education-how-does-a-persons-income-level-impact-the-ease-with-which-they-can-access-credit-under-your-decision-system",
    "href": "posts/BlogPost2/BlogPost2-2.html#what-about-people-seeking-loans-for-business-ventures-or-education-how-does-a-persons-income-level-impact-the-ease-with-which-they-can-access-credit-under-your-decision-system",
    "title": "Optimal Decision Making Blog Post",
    "section": "What about people seeking loans for business ventures or education? How does a person’s income level impact the ease with which they can access credit under your decision system?",
    "text": "What about people seeking loans for business ventures or education? How does a person’s income level impact the ease with which they can access credit under your decision system?\nIn answering this question, let’s first look at people with intents to take loans out for business ventures.\n\nVenture = X_train[X_train['loan_intent_VENTURE']].copy()\n\nVenture\n\n\n\n\n\n\n\n\nloan_intent_VENTURE\nloan_intent_EDUCATION\n\n\n\n\n1\nFalse\nTrue\n\n\n2\nFalse\nTrue\n\n\n3\nFalse\nFalse\n\n\n4\nFalse\nFalse\n\n\n6\nFalse\nFalse\n\n\n...\n...\n...\n\n\n26059\nFalse\nTrue\n\n\n26060\nTrue\nFalse\n\n\n26061\nFalse\nFalse\n\n\n26062\nFalse\nFalse\n\n\n26063\nFalse\nTrue\n\n\n\n\n22907 rows × 2 columns\n\n\n\nSimilar to previous summary tables, we group the incomes into bins in order to understand how a person’s income level might impact the accessibility to credit.We can borrow some of the necessary code from earlier.\n\n# Income bins\nbins = [0, 50000, 100000, 200000, float('inf')]  \n\n# Labels for bins\nlabels = ['0-50k', '50k-100k', '100k-200k', '200k+']\n\n# New column in data frame: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html\nVenture['income_bins'] = pd.cut(Venture['person_income'], bins=bins, labels=labels, right=False)\n\nsummary_table5 = Venture.groupby('income_bins', observed=True).agg({'loan_amnt': 'mean', 'Default Prob': 'mean'}).reset_index()\nsummary_table5.columns = [\"Income Bins\", \"Mean Loan Amnt\", 'Mean Default Prob']\n\nsummary_table5\n\n\n\n\n\n\n\n\nIncome Bins\nMean Loan Amnt\nMean Default Prob\n\n\n\n\n0\n0-50k\n7110.743034\n0.647473\n\n\n1\n50k-100k\n10320.122951\n0.820480\n\n\n2\n100k-200k\n13556.037736\n0.960362\n\n\n3\n200k+\n17455.241935\n0.999000\n\n\n\n\n\n\n\nNow, let’s do a similar analysis but for people who intend to use their loans for educational purposes.\n\nEducation = X_train[X_train['loan_intent_EDUCATION']].copy()\n\nEducation\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\n...\nincome_bins_0-50k\nincome_bins_50k-100k\nincome_bins_100k-200k\nincome_bins_200k+\nDefault Prob\nRepay Profit\nDefault Profit\nDefault Unlikely\nCorrect Assumption\nTotal Profit\n\n\n\n\n1\n27\n98000\n3.0\n11750\n0.1347\n0.12\n6\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\n0.938437\n4613.567568\n-6997.533847\n0\n1\n4613.567568\n\n\n2\n22\n36996\n5.0\n10000\n0.0751\n0.27\n4\nFalse\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nFalse\n0.607182\n2044.334031\n-6426.108799\n0\n1\n2044.334031\n\n\n16\n26\n88500\n10.0\n10000\n0.0859\n0.11\n4\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\n0.925889\n2367.371880\n-6341.815694\n0\n1\n2367.371880\n\n\n22\n37\n85000\n3.0\n10000\n0.1427\n0.12\n14\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\n0.915536\n4198.253635\n-5891.114794\n0\n1\n4198.253635\n\n\n27\n22\n53000\n2.0\n6500\n0.1065\n0.12\n4\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\n0.811192\n1953.405752\n-4016.866450\n0\n1\n1953.405752\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26043\n29\n48000\n5.0\n10000\n0.1273\n0.21\n7\nTrue\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nFalse\n0.707227\n3679.192509\n-6014.542748\n0\n1\n3679.192509\n\n\n26045\n40\n136000\n5.0\n21000\n0.0999\n0.15\n13\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\n0.963747\n5875.219617\n-13086.951569\n0\n1\n5875.219617\n\n\n26053\n24\n110000\n1.0\n11200\n0.0749\n0.10\n4\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\n0.963373\n2283.035050\n-7198.985446\n0\n1\n2283.035050\n\n\n26059\n36\n150000\n8.0\n3000\n0.0729\n0.02\n17\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\n0.996879\n593.840622\n-1932.967484\n0\n1\n593.840622\n\n\n26063\n25\n60000\n5.0\n21450\n0.0729\n0.36\n4\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\n0.537119\n4245.960448\n-13820.717511\n0\n0\n13820.717511\n\n\n\n\n4528 rows × 29 columns\n\n\n\n\n# Income bins\n#bins = [0, 50000, 100000, 200000, float('inf')]  \n\n# Labels for bins\n#labels = ['0-50k', '50k-100k', '100k-200k', '200k+']\n\n# New column in data frame: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html\nEducation['income_bins'] = pd.cut(Education['person_income'], bins=bins, labels=labels, right=False)\n\nsummary_table6 = Education.groupby('income_bins', observed=True).agg({'loan_amnt': 'mean', 'Default Prob': 'mean'}).reset_index()\nsummary_table6.columns = [\"Income Bins\", \"Mean Loan Amnt\", 'Mean Default Prob']\n\nsummary_table6\n\n\n\n\n\n\n\n\nIncome Bins\nMean Loan Amnt\nMean Default Prob\n\n\n\n\n0\n0-50k\n6936.549946\n0.658564\n\n\n1\n50k-100k\n10460.900815\n0.815266\n\n\n2\n100k-200k\n13602.153558\n0.955951\n\n\n3\n200k+\n17583.076923\n0.998853"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "CSCI 0451: Reflective Goal-Setting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal Decision Making Blog Post\n\n\n\n\n\nCredit Risk Prediction\n\n\n\n\n\nMay 15, 2024\n\n\nLucas Flemming\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\n\n\n\n\n\nImplementing Logistic Regression test\n\n\n\n\n\nMay 14, 2024\n\n\nLucas Flemming\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins Blog Post\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nMay 10, 2024\n\n\nLucas Flemming\n\n\n\n\n\n\n\n\n\n\n\n\nPerceptron Blog Post\n\n\n\n\n\nImplementing the Perceptron algorithm\n\n\n\n\n\nMay 10, 2024\n\n\nLucas Flemming\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]