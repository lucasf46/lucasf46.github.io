[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a test."
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/BlogPost2-2.html",
    "href": "posts/BlogPost2-2.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "PART A: Grab the Data\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\ndf_train.head(50)\n\nPART B: Explore the Data\n\n# SUMMARY TABLE\n\n#summary_table = df_train.groupby('loan_intent')['loan_int_rate'].agg(['mean', 'median']).reset_index()\n#summary_table.columns = ['Loan Intent', 'Mean Loan Interest Rate', 'Median Loan Interest Rate']\n\n#summary_table2 = df_train.groupby('person_home_ownership')['cb_person_cred_hist_length'].agg(['mean']).reset_index()\n#summary_table2.columns = ['Person Home Ownership', \"Mean Credit History Length\"]\n\n#print(summary_table)\n#print(summary_table2)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# TWO DATA VISUALIZATIONS\n\n#avg_age_by_intent = df_train.groupby(\"loan_intent\")['person_age'].mean().reset_index()\n#avg_employment_length_by_intent = df_train.groupby(\"loan_intent\")['person_emp_length'].mean().reset_index()\n#home_ownership = df_train.groupby(\"person_home_ownership\")['loan_int_rate'].mean().reset_index()\n\nplt.figure(figsize=(12, 20))\n\n# Modify dataset to remove datapoints with person_age &gt; 90 and person_employment_length &gt; 40\ndf_train = df_train[df_train['person_age'] &lt; 70]\ndf_train = df_train[df_train['person_emp_length'] &lt; 40]\n\nplt.subplot(3, 1, 1)\n\nsmaller_df = df_train.head(200)\nsns.boxplot(x= \"loan_intent\", y= \"person_age\", data= df_train)\n\nplt.subplot(3, 1, 2)\nsns.scatterplot(x= 'person_emp_length', y= 'loan_int_rate', hue= 'cb_person_default_on_file', data= df_train)\n\nplt.subplot(3, 1, 3)\nsns.scatterplot(x= 'person_income', y= 'loan_int_rate', hue= 'cb_person_default_on_file', data= df_train)\n#sns.barplot(x= \"person_home_ownership\", )\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ndf_train['cb_person_default_on_file'] = df_train['cb_person_default_on_file'].replace({'Y': 1, 'N': 0})\n\n# Code comes from: https://scikit-learn.org/stable/auto_examples/feature_selection/plot_select_from_model_diabetes.html#sphx-glr-auto-examples-feature-selection-plot-select-from-model-diabetes-py\nfrom sklearn.feature_selection import SequentialFeatureSelector\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\n# Import Linear Regression packages\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression(max_iter = 1000) \n\ndf_train.dropna(inplace=True)\n\nX = df_train.drop(['loan_intent', 'person_home_ownership', 'loan_grade', 'loan_status'], axis = 1)\n\ny = df_train['loan_status']\n\nPART C: Build a Model\n\n# Sequential Feature Selector (Cross-validation is built into this model)\n\nsfs_forward = SequentialFeatureSelector(\n    LR, n_features_to_select=3, direction=\"forward\"\n).fit(X, y)\n\n\nsfs_backward = SequentialFeatureSelector(\n    LR, n_features_to_select=3, direction=\"backward\"\n).fit(X, y)\n\n\nprint(\n    \"Features selected by forward sequential selection: \"\n    f\"{X.columns[sfs_forward.get_support()]}\"\n)\n\nprint(\n    \"Features selected by backward sequential selection: \"\n    f\"{X.columns[sfs_backward.get_support()]}\"\n)\n\nfcols = X.columns[sfs_forward.get_support()]\nbcols = X.columns[sfs_backward.get_support()]\n\nX_train = df_train[fcols]\ny_train = df_train[\"loan_status\"]\n\nX_train\n\nFeatures selected by forward sequential selection: Index(['person_emp_length', 'loan_int_rate', 'loan_percent_income'], dtype='object')\nFeatures selected by backward sequential selection: Index(['person_income', 'loan_amnt', 'cb_person_cred_hist_length'], dtype='object')\n\n\n\n\n\n\n\n\n\nperson_emp_length\nloan_int_rate\nloan_percent_income\n\n\n\n\n1\n3.0\n13.47\n0.12\n\n\n2\n5.0\n7.51\n0.27\n\n\n3\n2.0\n12.87\n0.05\n\n\n4\n2.0\n9.63\n0.28\n\n\n6\n2.0\n14.91\n0.25\n\n\n...\n...\n...\n...\n\n\n26059\n8.0\n7.29\n0.02\n\n\n26060\n1.0\n5.42\n0.09\n\n\n26061\n0.0\n11.71\n0.25\n\n\n26062\n12.0\n12.68\n0.24\n\n\n26063\n5.0\n7.29\n0.36\n\n\n\n\n22892 rows × 3 columns\n\n\n\n\n# X_train are feature columns (use a new dataframe with just those columns)\n\n# Fitting the Logistic Regression\nLR.fit(X_train, y_train)\n\ncoef = LR.coef_\n\n# LR.predict_proba(X_train) is equivalent to X_train@LR.coef_ via SciKitLearn / TA Bell\nprobs = LR.predict_proba(X_train)\n\n# Take the 0th index of each prob element which is the probability of a 0, via array slicing\n# This is the probability that the person did NOT default on their loan\nno_default_prob = probs[:, 0]\n\n\ncoef\n\narray([[-0.04430865,  0.28906601,  8.38643566]])\n\n\n\nprobs\n\n# 0 = DID NOT DEFAULT\n# 1 = DEFAULTED\n\n# probability that the person did not default\n\narray([[0.79520771, 0.20479229],\n       [0.87102954, 0.12897046],\n       [0.8882301 , 0.1117699 ],\n       ...,\n       [0.65525076, 0.34474924],\n       [0.72658701, 0.27341299],\n       [0.77187128, 0.22812872]])\n\n\n\nthreshold = 0.4\nnewDF = df_train[no_default_prob &gt; threshold]\nsortedDF = newDF.sort_values(by=['loan_status'])\nsortedDF\n#newDF['']\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\n1\n6\n\n\n16546\n24\n45000\nRENT\n1.0\nPERSONAL\nC\n2000\n13.99\n0\n0.04\n1\n2\n\n\n16544\n31\n96000\nMORTGAGE\n15.0\nHOMEIMPROVEMENT\nD\n8000\n14.09\n0\n0.08\n1\n9\n\n\n16543\n22\n35064\nRENT\n4.0\nDEBTCONSOLIDATION\nA\n2000\n8.38\n0\n0.06\n0\n2\n\n\n16542\n28\n62000\nRENT\n0.0\nHOMEIMPROVEMENT\nA\n15000\n7.90\n0\n0.24\n0\n10\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n21176\n26\n44000\nRENT\n5.0\nDEBTCONSOLIDATION\nC\n4600\n13.99\n1\n0.10\n0\n4\n\n\n3989\n26\n34000\nRENT\n2.0\nMEDICAL\nB\n13000\n9.63\n1\n0.38\n0\n2\n\n\n9029\n25\n39000\nRENT\n2.0\nVENTURE\nC\n8500\n13.23\n1\n0.22\n1\n4\n\n\n9028\n34\n54000\nRENT\n5.0\nMEDICAL\nD\n3000\n15.37\n1\n0.06\n1\n6\n\n\n26063\n25\n60000\nRENT\n5.0\nEDUCATION\nA\n21450\n7.29\n1\n0.36\n0\n4\n\n\n\n\n21265 rows × 12 columns\n\n\n\n\n#def calc_profit(default_prob, threshold, loan_amnts, loan_int_rate, y_train):\ny_train\n\n1        0\n2        0\n3        1\n4        0\n6        1\n        ..\n26059    0\n26060    0\n26061    0\n26062    0\n26063    1\nName: loan_status, Length: 22892, dtype: int64\n\n\n\ntotal_profits = []\n\n# Iterating through threshold values\nthresholds = np.linspace(0.01, 1, 100)\n\nmax_profit = 0\noptimal_threshold = 0\n\nfor threshold in thresholds:\n\n    # Defaults are the items with predict_proba of 0 is greater than the threshold, repayments have predict_proba for 0 that are less than threshold\n\n    # These are all the people expected to pack back their loans\n    People_given_loans = df_train[no_default_prob &gt; threshold]\n    \n    # Extract the loan amount/interest rate from each item categorized as a default\n    loan_amnt = People_given_loans['loan_amnt']\n    loan_int_rate = People_given_loans['loan_int_rate']\n\n    y = People_given_loans['loan_status']\n\n    # Profit formula if borrower defaults on loan\n    default_profit_formula = loan_amnt*(1 + 0.25*loan_int_rate)**3 - 1.7*loan_amnt\n    default_profit = default_profit_formula * (y) \n\n    # Profit formula if borrower repays loan\n    repayment_profit_formula = loan_amnt*(1 + 0.25*loan_int_rate)**10 - loan_amnt\n    repayment_profit = repayment_profit_formula * (1 - y)\n\n    total_profit_per_person = repayment_profit - default_profit\n    total_profit = total_profit_per_person.sum()\n    total_profits.append(total_profit)\n\n    if total_profit &gt; max_profit:\n        max_profit = total_profit\n        optimal_threshold = threshold\n\n    \n\n\n\n\n\n\n\n\n#print(\"The LR.coef is: \", LR.coef_)\n#print(\"Probs: \", probs)\n\n# Cross validate, use best scoring as parameters to grid CV\n# Retrieve weight vector, stored as an attribute\n# Predict.proba\n\nPART D: Find a Threshold\n\nimport seaborn as sns\n\nsns.scatterplot(x= thresholds, y= total_profits)\n\n\n\n\n\n\n\n\nEVALUATE MODEL FROM BANK’S PERSPECTIVE\nError rate vs profit\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)"
  },
  {
    "objectID": "posts/BlogPost3/BlogPost3-3.html",
    "href": "posts/BlogPost3/BlogPost3-3.html",
    "title": "Perceptron Blog Post",
    "section": "",
    "text": "LINK TO PERCEPTRON ON GITHUB:\nhttps://github.com/lucasf46/lucasf46.github.io/blob/main/posts/BlogPost3/perceptron.py\n\n\nINTRODUCTION:\nThe purpose of this blog post is to implement the perceptron algorithm and to understand how the algorithm runs with various types of data. In this particular case, there are three types of data the perceptron runs on: linearly separable data, non-linearly separable data, and data with more than 2 features. The first way to visualize the effectiveness of the perceptron is to track the algorithms calculation of loss as it searches for an ideal weight vector. A second way is to see the actual classification line shift on top of the graphed data.\n\n\nPERCEPTRON GRAD:\nThe perceptron.grad function takes the feature matrix X and the target vector y as arguments. First, the function computes the score for each element x in the data with a weight w. The value of y in this binary classification problem can be either -1 or 1. Therefore, within the function’s return statement, the predicted score of the ith element is multiplied the yi which is the true label of that ith element. If the value is less than 0, that means the predicted and actual values are different, indicating a misclassification. Multiplying that result by the product of X and y sets the elements corresponding to misclassified samples to their original values, while setting the correctly classified elements to zero.\n\n\nPart A: Implement Perceptron\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\nGRAPHING LOSS FUNCTION OVER TIME:\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\nmax_iterations = 100\niterations = 0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nloss_threshold = 0.01\n\n#while loss &gt; 0: # dangerous -- only terminates if data is linearly separable\nwhile iterations &lt; max_iterations:\n    # not part of the update: just for tracking our progress \n\n    if (loss &lt; loss_threshold):\n        break\n\n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n                \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n    iterations += 1\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\nVISUALIZING PERCEPTRON ALGORITHM OVER SEVERAL ITERATIONS\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np1 = Perceptron()\nopt1 = PerceptronOptimizer(p1)\np1.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p1.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    #i, local_loss = opt.step(X, y)\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n                \n    # perform a perceptron update using the random data point\n    local_loss = opt1.step(x_i, y_i)\n\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p1.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p1.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\nNON-LINEARLY SEPARABLE DATA\n\ndef new_perceptron_data(n_points = 300, noise = 0.5, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\n\nX, y = new_perceptron_data(n_points = 300, noise = 0.5)\n\n\ndef plot_new_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nThis new_perceptron_data is not linearly separable. Therefore, we expect the perceptron algorithm to not find a weight vector such that the loss is minimized to 0. The perceptron is run with this new_perceptron_data in the same way as before with the linearly separable data.\n\nfig, ax = plt.subplots(1, 1)\nX, y = new_perceptron_data()\nplot_new_perceptron_data(X, y, ax)\n\n\n# instantiate a model and an optimizer\np1 = Perceptron() \nopt1 = PerceptronOptimizer(p1)\n\nloss = 1.0\n\nmax_iterations = 1000\niterations = 0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nloss_threshold = 0.01\n\n#while loss &gt; 0: # dangerous -- only terminates if data is linearly separable\nwhile iterations &lt; max_iterations:\n    # not part of the update: just for tracking our progress \n\n    if (loss &lt; loss_threshold):\n        break\n\n    loss = p1.loss(X, y) \n    loss_vec.append(loss)\n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n                \n    # perform a perceptron update using the random data point\n    opt1.step(x_i, y_i)\n\n    iterations += 1\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \n#p = Perceptron()\n#opt = PerceptronOptimizer(p)\np1.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p1.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    #i, local_loss = opt.step(X, y)\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n                \n    # perform a perceptron update using the random data point\n    local_loss = opt1.step(x_i, y_i)\n\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_new_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p1.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p1.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\nBEYOND 2 DIMENSIONS\n\ndef five_dimension_perceptron_data(n_points = 300, noise = 0.2, p_dims = 5):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\n\nX, y = five_dimension_perceptron_data(n_points = 300, noise = 0.2)\n\n\n# instantiate a model and an optimizer\np3 = Perceptron() \nopt3 = PerceptronOptimizer(p3)\n\nloss = 1.0\n\nmax_iterations = 100\niterations = 0\n\n# for keeping track of loss values\nloss_vec3 = []\n\nn = X.size()[0]\n\nloss_threshold = 0.01\n\n#while loss &gt; 0: # dangerous -- only terminates if data is linearly separable\nwhile iterations &lt; max_iterations:\n    # not part of the update: just for tracking our progress \n\n    if (loss &lt; loss_threshold):\n        break\n\n    loss = p3.loss(X, y) \n    loss_vec3.append(loss)\n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n                \n    # perform a perceptron update using the random data point\n    opt3.step(x_i, y_i)\n\n    iterations += 1\n\n\nplt.plot(loss_vec3, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec3)), loss_vec3, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\nBased on the graph pictured above, the perceptron with &gt; 2 dimensions converged, finding a weight vector such that the loss value is minimized to 0. I believe the data is linearly separable considering that in testing the data, the perceptron converges well before reaching the max iteration limit of 1000.\nThe runtime complexity of the perceptron algorithm is O(n * d) where n is the number of data points.\n\n\nABSTRACT"
  },
  {
    "objectID": "posts/goal-setting.html",
    "href": "posts/goal-setting.html",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Lucas Flemming\n\n\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI am particularly interested in the Theory and Implementation. I would like to develop a strong foundation of exactly what is going on behind the scenes because until this course, Machine Learning has been somewhat of a mystery to me. I’m trying to not only understand Machine Learning concepts but apply them effectively in formats such as the blog posts but more importantly, in the final project which could incorporate real world data that carries significance to me (no offense to the Penguins on Dream Island).\n\n\n\n\n\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nI plan on seriously attempting every blog post as a way to familiarize myself with the various aspects of ML. I feel as though 3/4 of the blog posts is an ambitious, but achievable goal for me. It is ambitious especially because I’m on the baseball team and I’m in season so much of time is taken up in practices and games.\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nI think completing every warmup to the best of my ability is the standard, so I expect nothing less. Even if I struggle with a warm-up activity, I will make an honest effort and communicate my challenges to group members, that’s what they are there for.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nI have a former teammate who is running a startup focused on connecting researchers with grants called ATOM, so I feel like there could be an opportunity to integrate machine learning with that process. I would have to talk to him to see where ML would be most useful. I would communicate with my team members to create a loose schedule that could be adjusted to ensure the workload is shared. I’m also interested in doing something sports related because I’m passionate about professional/collegiate athletics and the project would feel less like work. I hope to use this project as something I can build upon in my time after Middlebury as I continue to look for future employment."
  },
  {
    "objectID": "posts/goal-setting.html#what-youll-learn",
    "href": "posts/goal-setting.html#what-youll-learn",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "The knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI am particularly interested in the Theory and Implementation. I would like to develop a strong foundation of exactly what is going on behind the scenes because until this course, Machine Learning has been somewhat of a mystery to me. I’m trying to not only understand Machine Learning concepts but apply them effectively in formats such as the blog posts but more importantly, in the final project which could incorporate real world data that carries significance to me (no offense to the Penguins on Dream Island)."
  },
  {
    "objectID": "posts/goal-setting.html#what-youll-achieve",
    "href": "posts/goal-setting.html#what-youll-achieve",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Most blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nI plan on seriously attempting every blog post as a way to familiarize myself with the various aspects of ML. I feel as though 3/4 of the blog posts is an ambitious, but achievable goal for me. It is ambitious especially because I’m on the baseball team and I’m in season so much of time is taken up in practices and games.\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nI think completing every warmup to the best of my ability is the standard, so I expect nothing less. Even if I struggle with a warm-up activity, I will make an honest effort and communicate my challenges to group members, that’s what they are there for.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nI have a former teammate who is running a startup focused on connecting researchers with grants called ATOM, so I feel like there could be an opportunity to integrate machine learning with that process. I would have to talk to him to see where ML would be most useful. I would communicate with my team members to create a loose schedule that could be adjusted to ensure the workload is shared. I’m also interested in doing something sports related because I’m passionate about professional/collegiate athletics and the project would feel less like work. I hope to use this project as something I can build upon in my time after Middlebury as I continue to look for future employment."
  },
  {
    "objectID": "posts/BlogPost1/BlogPost1-1.html",
    "href": "posts/BlogPost1/BlogPost1-1.html",
    "title": "Palmer Penguins Blog Post",
    "section": "",
    "text": "ABSTRACT\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\nDATA PREPARATION\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis = 1)\n    df = pd.get_dummies(df, dtype = float)\n    return df, y\n\n\nX_train, y_train = prepare_data(train)\n\nX_train\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\n51.1\n16.5\n225.0\n5250.0\n8.20660\n-26.36863\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n271\n35.9\n16.6\n190.0\n3050.0\n8.47781\n-26.07821\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n272\n39.5\n17.8\n188.0\n3300.0\n9.66523\n-25.06020\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n\n\n273\n36.7\n19.3\n193.0\n3450.0\n8.76651\n-25.32426\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n\n\n274\n42.4\n17.3\n181.0\n3600.0\n9.35138\n-24.68790\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n\n\n\n\n256 rows × 14 columns\n\n\n\n\n\nEXPLORE:\nCreate two visualizations of the data and one summary table.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/palmer-penguins.csv\"\ndf = pd.read_csv(url)\n\nSumTable = df.groupby(['Species', 'Sex']).aggregate({'Body Mass (g)': 'mean'})\nSumTable2 = df.groupby(['Species', 'Island']).aggregate({'Culmen Length (mm)': 'mean'})\n\n# Filter to get penguins living on Dream Island\nIsland_Dream_df = df[df['Island'] == 'Dream']\n\nprint(SumTable, SumTable2)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n# Plot first scatterplot\nsns.scatterplot(data=df, x='Culmen Length (mm)', y='Culmen Depth (mm)', hue= 'Species', ax=axes[0])\naxes[0].set_title('Scatterplot 1')\n\n# Plot second scatterplot\nsns.scatterplot(x='Culmen Length (mm)', y='Culmen Depth (mm)', data= Island_Dream_df, hue= 'Species', ax=axes[1])\naxes[1].set_title('Scatterplot 2')\n\n# Adjust layout\nplt.tight_layout()\n\n# Show plot\nplt.show()\n\n                                                  Body Mass (g)\nSpecies                                   Sex                  \nAdelie Penguin (Pygoscelis adeliae)       FEMALE    3368.835616\n                                          MALE      4043.493151\nChinstrap penguin (Pygoscelis antarctica) FEMALE    3527.205882\n                                          MALE      3938.970588\nGentoo penguin (Pygoscelis papua)         .         4875.000000\n                                          FEMALE    4679.741379\n                                          MALE      5484.836066                                                      Culmen Length (mm)\nSpecies                                   Island                       \nAdelie Penguin (Pygoscelis adeliae)       Biscoe              38.975000\n                                          Dream               38.501786\n                                          Torgersen           38.950980\nChinstrap penguin (Pygoscelis antarctica) Dream               48.833824\nGentoo penguin (Pygoscelis papua)         Biscoe              47.504878\n\n\n\n\n\n\n\n\n\n\n\nFEATURE SELECTION:\n\nfrom itertools import combinations\nimport numpy as np\n\n# Alternative tools to select features\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\n# Types of models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Optimizing max_depth\nfrom sklearn.model_selection import GridSearchCV\n\n# Cross validation\nfrom sklearn.model_selection import cross_val_score\n\n# Qualitative and quantitative columns I've selected\nall_qual_columns = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Stage_Adult, 1 Egg Stage\",\"Clutch Completion_No\", \"Clutch Completion_Yes\", \"Sex_FEMALE\", \"Sex_MALE\" ]\nall_quant_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\nLR = LogisticRegression(max_iter = 1000)  \n\nclf = DecisionTreeClassifier(random_state=0)\nparameters = {'max_depth': np.arange(1, 21)}\n\nall_cols = []\n\nfor qual in all_qual_columns: \n    qual_cols = [col for col in X_train.columns if qual in col] \n    for pair in combinations(all_quant_columns, 2):\n        cols =  list(pair) + qual_cols \n\n        # Linear Regression Model\n\n        LR.fit(X_train[cols], y_train)\n        LR_accuracy = LR.score(X_train[cols], y_train)\n        cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n\n        # Decision Tree Classifier Model\n\n        grid_search = GridSearchCV(clf, parameters, cv = 5)\n        grid_search.fit(X_train[cols], y_train)\n        best_clf = grid_search.best_estimator_\n        best_params = grid_search.best_params_\n        dt_accuracy = best_clf.score(X_train[cols], y_train)\n        cv_scores_DecisionTree = cross_val_score(best_clf, X_train[cols], y_train, cv = 5)\n        \n        if (dt_accuracy == 1.0):\n            print(f\"Features: {cols}, LR Accuracy: {LR_accuracy}, DT Accuracy: {dt_accuracy}, Best Params: {best_params}\")\n        \n\nFeatures: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Dream'], LR Accuracy: 0.99609375, DT Accuracy: 1.0, Best Params: {'max_depth': 5}\nFeatures: ['Body Mass (g)', 'Delta 13 C (o/oo)', 'Island_Dream'], LR Accuracy: 0.82421875, DT Accuracy: 1.0, Best Params: {'max_depth': 8}\nFeatures: ['Culmen Length (mm)', 'Body Mass (g)', 'Sex_FEMALE'], LR Accuracy: 0.97265625, DT Accuracy: 1.0, Best Params: {'max_depth': 7}\nFeatures: ['Culmen Length (mm)', 'Body Mass (g)', 'Sex_MALE'], LR Accuracy: 0.97265625, DT Accuracy: 1.0, Best Params: {'max_depth': 7}\n\n\n\n\nTESTING\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest_clf = DecisionTreeClassifier(random_state=0, max_depth= 7)\nX_test, y_test = prepare_data(test)   \n\n\ntest_cols = ['Culmen Length (mm)', 'Body Mass (g)', 'Sex_MALE', 'Sex_FEMALE']\n\n\ntest_clf = test_clf.fit(X_test[test_cols], y_test)\n\n\nTest_score = test_clf.score(X_test[test_cols], y_test)\n\n\nX_test[test_cols]\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nBody Mass (g)\nSex_MALE\nSex_FEMALE\n\n\n\n\n0\n41.7\n4700.0\n0.0\n1.0\n\n\n1\n50.7\n4050.0\n1.0\n0.0\n\n\n2\n38.1\n3425.0\n0.0\n1.0\n\n\n3\n39.6\n3550.0\n0.0\n1.0\n\n\n4\n43.3\n4400.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n\n\n64\n42.5\n3350.0\n0.0\n1.0\n\n\n65\n54.3\n5650.0\n1.0\n0.0\n\n\n66\n59.6\n6050.0\n1.0\n0.0\n\n\n67\n36.9\n3500.0\n0.0\n1.0\n\n\n68\n49.6\n4750.0\n1.0\n0.0\n\n\n\n\n68 rows × 4 columns\n\n\n\n\n\nPLOTTING DECISION REGIONS\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(test_clf, X_test[test_cols], y_test)\n\n\n\n\n\n\n\n\n\n\nCONFUSION MATRIX\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = test_clf.predict(X_test[test_cols])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\n\n\nDISCUSSION"
  },
  {
    "objectID": "posts/BlogPost1-1.html",
    "href": "posts/BlogPost1-1.html",
    "title": "Palmer Penguins Blog Post",
    "section": "",
    "text": "ABSTRACT\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\nDATA PREPARATION\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis = 1)\n    df = pd.get_dummies(df, dtype = float)\n    return df, y\n\n\nX_train, y_train = prepare_data(train)\n\nX_train\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\n51.1\n16.5\n225.0\n5250.0\n8.20660\n-26.36863\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n271\n35.9\n16.6\n190.0\n3050.0\n8.47781\n-26.07821\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n272\n39.5\n17.8\n188.0\n3300.0\n9.66523\n-25.06020\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n\n\n273\n36.7\n19.3\n193.0\n3450.0\n8.76651\n-25.32426\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n\n\n274\n42.4\n17.3\n181.0\n3600.0\n9.35138\n-24.68790\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n\n\n\n\n256 rows × 14 columns\n\n\n\n\n\nEXPLORE:\nCreate two visualizations of the data and one summary table.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/palmer-penguins.csv\"\ndf = pd.read_csv(url)\n\nSumTable = df.groupby(['Species', 'Sex']).aggregate({'Body Mass (g)': 'mean'})\nSumTable2 = df.groupby(['Species', 'Island']).aggregate({'Culmen Length (mm)': 'mean'})\n\n# Filter to get penguins living on Dream Island\nIsland_Dream_df = df[df['Island'] == 'Dream']\n\nprint(SumTable, SumTable2)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n# Plot first scatterplot\nsns.scatterplot(data=df, x='Culmen Length (mm)', y='Culmen Depth (mm)', hue= 'Species', ax=axes[0])\naxes[0].set_title('Scatterplot 1')\n\n# Plot second scatterplot\nsns.scatterplot(x='Culmen Length (mm)', y='Culmen Depth (mm)', data= Island_Dream_df, hue= 'Species', ax=axes[1])\naxes[1].set_title('Scatterplot 2')\n\n# Adjust layout\nplt.tight_layout()\n\n# Show plot\nplt.show()\n\n                                                  Body Mass (g)\nSpecies                                   Sex                  \nAdelie Penguin (Pygoscelis adeliae)       FEMALE    3368.835616\n                                          MALE      4043.493151\nChinstrap penguin (Pygoscelis antarctica) FEMALE    3527.205882\n                                          MALE      3938.970588\nGentoo penguin (Pygoscelis papua)         .         4875.000000\n                                          FEMALE    4679.741379\n                                          MALE      5484.836066                                                      Culmen Length (mm)\nSpecies                                   Island                       \nAdelie Penguin (Pygoscelis adeliae)       Biscoe              38.975000\n                                          Dream               38.501786\n                                          Torgersen           38.950980\nChinstrap penguin (Pygoscelis antarctica) Dream               48.833824\nGentoo penguin (Pygoscelis papua)         Biscoe              47.504878\n\n\n\n\n\n\n\n\n\n\n\nFEATURE SELECTION:\n\nfrom itertools import combinations\nimport numpy as np\n\n# Alternative tools to select features\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\n# Types of models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Optimizing max_depth\nfrom sklearn.model_selection import GridSearchCV\n\n# Cross validation\nfrom sklearn.model_selection import cross_val_score\n\n# Qualitative and quantitative columns I've selected\nall_qual_columns = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Stage_Adult, 1 Egg Stage\",\"Clutch Completion_No\", \"Clutch Completion_Yes\", \"Sex_FEMALE\", \"Sex_MALE\" ]\nall_quant_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\nLR = LogisticRegression(max_iter = 1000)  \n\nclf = DecisionTreeClassifier(random_state=0)\nparameters = {'max_depth': np.arange(1, 21)}\n\nall_cols = []\n\nfor qual in all_qual_columns: \n    qual_cols = [col for col in X_train.columns if qual in col] \n    for pair in combinations(all_quant_columns, 2):\n        cols =  list(pair) + qual_cols \n\n        # Linear Regression Model\n\n        LR.fit(X_train[cols], y_train)\n        LR_accuracy = LR.score(X_train[cols], y_train)\n        cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n\n        # Decision Tree Classifier Model\n\n        grid_search = GridSearchCV(clf, parameters, cv = 5)\n        grid_search.fit(X_train[cols], y_train)\n        best_clf = grid_search.best_estimator_\n        best_params = grid_search.best_params_\n        dt_accuracy = best_clf.score(X_train[cols], y_train)\n        cv_scores_DecisionTree = cross_val_score(best_clf, X_train[cols], y_train, cv = 5)\n        \n        if (dt_accuracy == 1.0):\n            print(f\"Features: {cols}, LR Accuracy: {LR_accuracy}, DT Accuracy: {dt_accuracy}, Best Params: {best_params}\")\n        \n\nFeatures: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Dream'], LR Accuracy: 0.99609375, DT Accuracy: 1.0, Best Params: {'max_depth': 5}\nFeatures: ['Body Mass (g)', 'Delta 13 C (o/oo)', 'Island_Dream'], LR Accuracy: 0.82421875, DT Accuracy: 1.0, Best Params: {'max_depth': 8}\nFeatures: ['Culmen Length (mm)', 'Body Mass (g)', 'Sex_FEMALE'], LR Accuracy: 0.97265625, DT Accuracy: 1.0, Best Params: {'max_depth': 7}\nFeatures: ['Culmen Length (mm)', 'Body Mass (g)', 'Sex_MALE'], LR Accuracy: 0.97265625, DT Accuracy: 1.0, Best Params: {'max_depth': 7}\n\n\n\n\nTESTING\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest_clf = DecisionTreeClassifier(random_state=0, max_depth= 7)\nX_test, y_test = prepare_data(test)   \n\n\ntest_cols = ['Culmen Length (mm)', 'Body Mass (g)', 'Sex_MALE', 'Sex_FEMALE']\n\n\ntest_clf = test_clf.fit(X_test[test_cols], y_test)\n\n\nTest_score = test_clf.score(X_test[test_cols], y_test)\n\n\nX_test[test_cols]\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nBody Mass (g)\nSex_MALE\nSex_FEMALE\n\n\n\n\n0\n41.7\n4700.0\n0.0\n1.0\n\n\n1\n50.7\n4050.0\n1.0\n0.0\n\n\n2\n38.1\n3425.0\n0.0\n1.0\n\n\n3\n39.6\n3550.0\n0.0\n1.0\n\n\n4\n43.3\n4400.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n\n\n64\n42.5\n3350.0\n0.0\n1.0\n\n\n65\n54.3\n5650.0\n1.0\n0.0\n\n\n66\n59.6\n6050.0\n1.0\n0.0\n\n\n67\n36.9\n3500.0\n0.0\n1.0\n\n\n68\n49.6\n4750.0\n1.0\n0.0\n\n\n\n\n68 rows × 4 columns\n\n\n\n\n\nPLOTTING DECISION REGIONS\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(test_clf, X_test[test_cols], y_test)\n\n\n\n\n\n\n\n\n\n\nCONFUSION MATRIX\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = test_clf.predict(X_test[test_cols])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\n\n\nDISCUSSION"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "CSCI 0451: Reflective Goal-Setting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins Blog Post\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nJan 10, 2023\n\n\nLucas Flemming\n\n\n\n\n\n\n\n\n\n\n\n\nPerceptron Blog Post\n\n\n\n\n\nImplementing the Perceptron algorithm\n\n\n\n\n\nJan 10, 2023\n\n\nLucas Flemming\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]